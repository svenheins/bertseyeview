{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "incoming-pizza",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "import configparser\n",
    "\n",
    "config_path = './input/config.ini'\n",
    "## general config\n",
    "config = configparser.ConfigParser()\n",
    "config.read(config_path)\n",
    "project_name = config['GENERAL-settings']['project_name']\n",
    "\n",
    "def write_results_to_csv(prefix_filename, topic, number_of_results, label, field, operator, dict_entities):\n",
    "    terms = \"\"\n",
    "    for key in dict_entities:\n",
    "        terms = terms + dict_entities[key]+\",\"\n",
    "    terms=terms[:-1]\n",
    "    terms_url = urllib.parse.quote(terms)    \n",
    "    dict_entities_inv = {v: k for k, v in dict_entities.items()}\n",
    "    \n",
    "    api_url = 'http://knowledge-graph-api-'+project_name+':5000/api/v1/top_n_articles_for_label?count='+str(number_of_results)+'&norm_by_age=true&label='+label+'&field='+field+'&operator='+operator+'&terms='+terms_url\n",
    "\n",
    "    #print(api_url)\n",
    "    r = requests.get(api_url)\n",
    "    literature_results = r.json()['results']\n",
    "    \n",
    "    f = open(prefix_filename+topic+'.csv', 'w')\n",
    "    if len(dict_entities) <= 1:\n",
    "        literature_results = [literature_results]\n",
    "    \n",
    "    str_output_header = \"\"\n",
    "    for key in literature_results[0][0]:\n",
    "        str_output_header= str_output_header + str(key) + \"|\"\n",
    "    str_output = str_output_header + \"term_label\"\n",
    "            \n",
    "    for i in range(len(dict_entities)):\n",
    "        for entry in literature_results[i]:\n",
    "            temp_str_output= \"\"\n",
    "            ## if there are no results, the entry \"message: ... no reults\" appears\n",
    "            if entry == \"message\":\n",
    "                break\n",
    "            for key in entry:\n",
    "                #print(\"key = \" + str(key) +\" entry = \"+ str(entry))\n",
    "                temp_str_output = temp_str_output + str(entry[key]).replace(\"|\",\"/\") +\"|\"\n",
    "            temp_str_output = temp_str_output + str(dict_entities_inv[entry[\"term\"]])\n",
    "            str_output = str_output + \"\\n\"+ temp_str_output\n",
    "    f.write(str_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "executed-database",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'results': [{'labels': {'Article': 67008, 'GO_BP': 11173, 'GO_CC': 1414, 'GO_MF': 3161, 'Keyword': 52, 'cellline': 117, 'chemical': 4114, 'disease': 4041, 'gene': 9096, 'pathway': 2047, 'pathway_biocarta': 251, 'pathway_kegg': 635, 'pathway_netpath': 35, 'pathway_pid': 210, 'pathway_reactome': 6301, 'pathway_wikipathways': 808, 'species': 1248}}]}\n"
     ]
    }
   ],
   "source": [
    "r = requests.get('http://knowledge-graph-api-'+project_name+':5000/api/v1/statistics')\n",
    "print(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-lottery",
   "metadata": {},
   "source": [
    "## Manual version (POC for ALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "gross-premium",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_filename = project_name+'_literature_'\n",
    "\n",
    "number_of_results = 2\n",
    "topic = 'cirrhosis_auoimmune_liver_diseases'\n",
    "dict_diseases = {\n",
    "    #\"ALS\":\"Disease:MESH:D000690\",\n",
    "    #\"ALS\":\"Disease:MESH:D000690\",\n",
    "    #\"TDP-43-linked ALS\":\"Disease:MESH:D057177\",\n",
    "    \n",
    "    \"cirrhosis\": \"Disease:MESH:D005355\",\n",
    "    \"autoimmune liver disease\": \"Disease:MESH:D008107\",\n",
    "}\n",
    "label = 'disease'\n",
    "field = 'name'\n",
    "operator = urllib.parse.quote(\"=\")\n",
    "\n",
    "## process query and write output-file\n",
    "write_results_to_csv(prefix_filename = prefix_filename, topic = topic, number_of_results=number_of_results, label=label, field=field, operator = operator, dict_entities = dict_diseases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-saturday",
   "metadata": {},
   "source": [
    "# Automatic version\n",
    "For each label_class (disease, gene, article, chemical, Keyword) retrieve the top instances. For those top instances / entities get the most relevant articles of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "charitable-douglas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class = disease took 0:00:02.523625 (hours:minutes:seconds.milliseconds)\n",
      "Class = gene took 0:00:01.727235 (hours:minutes:seconds.milliseconds)\n",
      "Class = chemical took 0:00:02.211099 (hours:minutes:seconds.milliseconds)\n",
      "Class = pathway_kegg took 0:00:10.550480 (hours:minutes:seconds.milliseconds)\n",
      "Class = pathway_reactome took 0:00:07.743417 (hours:minutes:seconds.milliseconds)\n",
      "Class = GO_BP took 0:00:07.743251 (hours:minutes:seconds.milliseconds)\n",
      "Class = species took 0:00:02.621825 (hours:minutes:seconds.milliseconds)\n",
      "The API took 0:00:35.122058 (hours:minutes:seconds.milliseconds) in order to get 5 for each of 30 entities for the classes: ['disease', 'gene', 'chemical', 'pathway_kegg', 'pathway_reactome', 'GO_BP', 'species']\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "## timing\n",
    "now_start = datetime.now()\n",
    "\n",
    "prefix_filename = 'literature_'\n",
    "label_classes = [\"disease\", \"gene\", \"chemical\", \"pathway_kegg\", \"pathway_reactome\",\"GO_BP\", \"species\"] # Keyword if there are special Keywords\n",
    "# for each label_class get the top top_count_entities entities\n",
    "top_count_entities = 30\n",
    "# for each of the top entities get number_of_results articles\n",
    "number_of_results = 5\n",
    "# use the field \"name\" as an identifier\n",
    "field = 'name'\n",
    "# exact match \"=\" instead of \"CONTAINS\"\n",
    "operator = urllib.parse.quote(\"=\")\n",
    "                 \n",
    "for label_class in label_classes:\n",
    "    now_start_class = datetime.now()\n",
    "    api_top_entities_for_label = \"http://knowledge-graph-api-\"+project_name+\":5000/api/v1/top_entities?count=\"+str(top_count_entities)+\"&label=\"+label_class\n",
    "    r = requests.get(api_top_entities_for_label)\n",
    "    label_results = r.json()['results']\n",
    "    dict_top_entities = {}\n",
    "    for entry in label_results:\n",
    "        dict_top_entities[entry['label']] = entry['name']    \n",
    "    write_results_to_csv(prefix_filename = prefix_filename, topic = label_class, number_of_results=number_of_results, label=label_class, field=field, operator = operator, dict_entities = dict_top_entities)       \n",
    "    now_end_class = datetime.now()\n",
    "    print(\"Class = \"+label_class + \" took \"+ str(now_end_class - now_start_class)+\" (hours:minutes:seconds.milliseconds)\")\n",
    "\n",
    "now_end = datetime.now()\n",
    "print(\"The API took \"+ str(now_end - now_start)+\" (hours:minutes:seconds.milliseconds) in order to get \"+str(number_of_results)+\" for each of \"+str(top_count_entities)+ \" entities for the classes: \"+str(label_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-convention",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
